{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过朴素贝叶斯算法进行文本分类"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "选择的机器学习库是sklearn\n",
    "\n",
    "文本分类分为5步\n",
    "1.对文档进行分词，并且给文档指定分类标签\n",
    "  英文文档中，最常用的是 NTLK 包\n",
    "  中文文档中，最常用的是 jieba 包\n",
    "\n",
    "2.计算单词的TF-IDF值，这是词频和逆向文档频率的乘积\n",
    "  文本分类中把每个单词当作一个特征，每个单词计算的TF-IDF值视为分类的特征值\n",
    "  \n",
    "3.加载停用词，指定在分类中没有用的词\n",
    "  计算单词TF-TDF值和加载停用词都是用到 sklearn 里的 TfidfVectorizer类\n",
    "  \n",
    "4生成朴素贝叶斯分类器\n",
    "\n",
    "5使用生成的分类器做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义分次函数\n",
    "def cut_words(file_path):\n",
    "    \"\"\"\n",
    "    对文本进行切词\n",
    "    :param file_path: txt文本路径\n",
    "    :return: 用空格分词的字符串\n",
    "    \"\"\"\n",
    "    text_with_spaces = ''\n",
    "    text=open(file_path, 'r', encoding='gb18030').read()\n",
    "    textcut = jieba.cut(text)\n",
    "    for word in textcut:\n",
    "        text_with_spaces += word + ' '\n",
    "    return text_with_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件加载函数\n",
    "def loadfile(file_dir):\n",
    "    \"\"\"\n",
    "    将路径下的所有文件加载\n",
    "    :param file_dir: 保存txt文件目录\n",
    "    :return: 分词后的文档列表和标签\n",
    "    注意每个文档对应一个label标签\n",
    "    \"\"\"\n",
    "    words_list = []\n",
    "    labels_list = []\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            label=root.split('/')[-1]\n",
    "            file_path = os.path.join(root, file)\n",
    "            words_list.append(cut_words(file_path))\n",
    "            labels_list.append(label) \n",
    "    return words_list, labels_list     \n",
    "        \n",
    "#     for file in file_list:\n",
    "#         file_path = file_dir + '/' + file\n",
    "#         words_list.append(cut_words(file_path))\n",
    "#         labels_list.append(label)                                                                                                                 \n",
    "#     return words_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.864 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#第一步：对文档进行分词\n",
    "train_documents, train_labels = loadfile(r'../data/text classification/train/')\n",
    "test_documents, test_labels = loadfile(r'../data/text classification/test/')\n",
    "# print(train_documents)\n",
    "# print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#第二步：加载停用词\n",
    "STOP_WORDS = [line.strip() for line in open(r'../data/text classification/stop/stopword.txt' ,errors='ignore',encoding='utf-8').readlines()]\n",
    "# print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#第三步：计算单词的权重\n",
    "tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)\n",
    "train_features = tf.fit_transform(train_documents)\n",
    "\n",
    "#第四步：生成朴素贝叶斯分类器\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n",
    "\n",
    "#第五步：使用生成的分类器做预测\n",
    "test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tf.vocabulary_)\n",
    "test_features = test_tf.fit_transform(test_documents)\n",
    "\n",
    "predict_labels = clf.predict(test_features)\n",
    "\n",
    "#第六步：计算准确率\n",
    "print(metrics.accuracy_score(test_labels, predict_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单独对某个文档进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文学']\n"
     ]
    }
   ],
   "source": [
    "file=r'../data/text classification/test/文学/780.txt'\n",
    "#过程同样是需要先进行分词，然后计算TF-TDF值\n",
    "testword_list=[]\n",
    "testword_list.append(cut_words(file))\n",
    "\n",
    "test = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tf.vocabulary_)\n",
    "features = test.fit_transform(testword_list)\n",
    "predict = clf.predict(features)\n",
    "print(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
